

# class ADVI:
#     def __init__(self, prior_dist, likelihood_log_prob_fun, data):
#         self.prior_dist = prior_dist
#         self.likelihood_log_prob_fun = likelihood_log_prob_fun
#         self.data = data

#         # parallelize the objective function
#         self._objective_fun_vmap = jax.vmap(self._objective_fun_per_sample, in_axes=(0, None, None))

#         # Define value and grad function
#         self.value_and_grad_fun = jax.jit(jax.value_and_grad(self.objective_fun, argnums=1), static_argnums=3)

#     def log_prior_likelihood(self, sample, data, **dists):
#         log_prior = self.prior_dist.log_prob(sample)
#         log_likelihood = self.likelihood_log_prob_fun(sample, data, **dists)
#         return log_prior + log_likelihood

#     def _objective_fun_per_sample(self, key, dists, data):
#         variational_dist = dists["variational_dist"]
#         sample, q_prob = variational_dist.sample_and_log_prob(key)
#         p_prob = self.log_prior_likelihood(sample, data, **dists)

#         return q_prob - p_prob

#     def objective_fun(self, key, dists, data, n_samples=1):
#         keys = jax.random.split(key, n_samples)
#         return self._objective_fun_vmap(keys, dists, data).mean()


# Another version


# class ADVI_V1:
#     def __init__(self, prior_dist, likelihood_log_prob_fun, bijector, data):
#         self.prior_dist = prior_dist
#         self.likelihood_log_prob_fun = likelihood_log_prob_fun
#         self.bijector = bijector
#         self.data = data

#         # parallelize the objective function
#         self._objective_fun_vmap = jax.vmap(self._objective_fun_per_sample, in_axes=(0, None))

#     def log_prior_likelihood(self, normal_sample):
#         transformed_sample = self.bijector.forward(normal_sample)
#         transformed_prior_dist = distrax.Transformed(self.prior_dist, distrax.Inverse(self.bijector))

#         log_prior = transformed_prior_dist.log_prob(normal_sample)
#         log_likelihood = self.likelihood_log_prob_fun(transformed_sample, self.data)
#         return log_prior + log_likelihood

#     def _objective_fun_per_sample(self, key, variational_dist):
#         normal_sample, q_prob = variational_dist.sample_and_log_prob(key)
#         p_prob = self.log_prior_likelihood(normal_sample)

#         return q_prob - p_prob

#     def objective_fun(self, key, variational_dist, n_samples=1):
#         keys = jax.random.split(key, n_samples)
#         return self._objective_fun_vmap(keys, variational_dist).mean()